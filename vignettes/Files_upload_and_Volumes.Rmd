---
title: "Files upload and Volumes in Seven Bridges API R Client"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: false
    theme: "flatly"
    highlight: "textmate"
    css: "sevenbridges.css"
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Files upload and download and Volumes in Seven Bridges API R Client}
  %\VignetteEncoding{UTF-8}
---

<a name="top"></a>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

# Files upload and download


## Upload single file

You can upload files from your local computer to the Platform using the 
`upload()` method on your `Auth` object. The method allows you to upload only
a single file. 

To upload a file you should provide its full path on your local computer
as the `path` parameter.

To specify upload destination for your file you can use either `project` or
`parent` parameter. These two parameters should not be used together.

* **project** - `Project` object or project ID.
* **parent** - `File` object (of type `Folder`) or its ID.

#### Upload a single file

```{r}
a <- Auth$new(platform = "aws-us", token = "<your-token>")


# Option 1 - Using the project parameter
# Option 1.a (providing a Project object as project parameter)
destination_project <- a$projects$get(project = "<short-name-of-the-project>")

up <- a$upload(
  path = "/path/to/your/file.txt",
  project = destination_project,
  overwrite = TRUE
)
# Call start() method do start upload process
up$start()

# Option 1.b (providing a project's ID as project parameter)
up <- a$upload(
  path = "/path/to/your/file.txt",
  project = "<project_id>",
  overwrite = TRUE
)
# Call start() method do start upload process
up$start()


# Option 2 - Using the parent parameter
# Option 2.a (providing a File (must be a folder) object as parent parameter)
destination_folder <- a$files$get(id = "<folder_id>")

up <- a$upload(
  path = "/path/to/your/file.txt",
  parent = destination_folder,
  overwrite = TRUE
)
# Call start() method do start upload process
up$start()

# Option 2.b (providing a file's (folder's) ID as project parameter)
up <- a$upload(
  path = "/path/to/your/file.txt",
  parent = "<destination_folder_id>",
  overwrite = TRUE
)
# Call start() method do start upload process
up$start()
```

If you want to skip the step where you need to call the `start()` method
to start the actual upload process, you can use the `init` parameter
when calling the `upload()` method. If `init` parameter is set to `FALSE`
the upload process will start right away.

```{r}
a <- Auth$new(platform = "aws-us", token = "<your-token>")

destination_project <- a$projects$get(project = "<short-name-of-the-project>")

up <- a$upload(
  path = "/path/to/your/file.txt",
  project = destination_project,
  overwrite = TRUE,
  init = FALSE
)
```

#### List all ongoing uploads

The method `list_ongoing_uploads()` allows you to see the list of all
ongoing upload processes.

```{r}
# List ongoing uploads
a$list_ongoing_uploads()
```

#### Abort upload process

You can abort any uploading process using the `upload_abort()` method.
To do so, you need to provide the ID of a process within the `upload_id`
parameter.

```{r}
# Abort upload
a$abort_upload(upload_id = "<id_of_the_upload_process>")
```


### Download a file

`File` object has a `download()` method, which allows you to download that file
to your local computer. You should provide the `directory_path` parameter, 
which specifies the destination directory to which your file will be 
downloaded. By default, this parameter is set to your current working
directory. You can also set the new name for your resulting (downloaded)
file by providing the `filename` parameter. Otherwise, the default name 
(the one stored in the `name` field of your `File` object) will be used.

```{r}
# Download a file
file$download(directory_path = "/path/to/your/destination/folder")
```


## Volumes

### List volumes

You can list all volumes you've registered by calling the `volumes$query()` 
method from the authentication object. The method doesn't have any additional 
query parameters that could allow you to search for volumes by specific 
criteria, except to control the number of results returned using limit and 
offset parameters.

```{r}
# Query volumes
a$volumes$query()
```

### Get single volume information

In order to retrieve information about a single volume of interest, you can get 
it using the `volumes$get()` method using its id as parameter.
Volume ID is usually presented in form '<division_name>/<volume_name>' for 
enterprise users, while for others it can be in form 
'<volume_owner>/<volume_name>'.

```{r}
# Get volume
a$volumes$get(id = "<division_name>/<volume_name>")
```

### Create volumes - AWS (s3) using IAM User authentication type

For creating volumes we have exposed several functions for different cloud 
providers and authentication types:
* `create_s3_using_iam_user` : creates s3 volume using IAM User authentication type
* `create_s3_using_iam_role`: creates s3 volume using IAM Role authentication type
* `create_google_using_iam_user` : creates GC volume using IAM User authentication type
* `create_google_using_iam_role`: creates GC volume using IAM Role authentication type
* `create_azure` : creates Azure volume (only RO privileges allowed)
* `create_ali_oss`: creates Ali cloud volume (only RO privileges allowed)

For each of the functions it is possible to provide parameters via path 
(`from_path`) to some JSON file where all required fields should be listed.

The examples of usages are listed below:

```{r}
# Create AWS volume using IAM User authentication type
aws_iam_user_volume <- a$volumes$create_s3_using_iam_user(
  name = "my_new_aws_user_volume",
  bucket = "<bucket-name>",
  description = "AWS IAM User volume",
  access_key_id = "<access-key>",
  secret_access_key = "<secret-access-key>"
)

aws_iam_user_volume_from_path <- a$volumes$create_s3_using_iam_user(
  from_path = "path/to/my/json/file.json"
)


# Create AWS volume using IAM Role authentication type
aws_iam_role_volume <- a$volumes$create_s3_using_iam_role(
  name = "my_new_aws_role_volume",
  bucket = "<bucket-name>",
  description = "AWS IAM Role volume",
  role_arn = "<role-arn-key>",
  external_id = "<external-id>"
)

aws_iam_role_volume_from_path <- a$volumes$create_s3_using_iam_role(
  from_path = "path/to/my/json/file.json"
)

# Create Google cloud volume using IAM User authentication type
gc_iam_user_volume <- a$volumes$create_google_using_iam_user(
  name = "my_new_gc_user_volume",
  access_mode = "RW",
  bucket = "<bucket-name>",
  description = "GC IAM User volume",
  client_email = "<client_email>",
  private_key = "<private_key-string>"
)

gc_iam_user_volume_from_path <- a$volumes$create_google_using_iam_user(
  from_path = "path/to/my/json/file.json"
)

# Create Google cloud volume using IAM Role authentication type
# by passing configuration parameter as named list
gc_iam_role_volume <- a$volumes$create_google_using_iam_role(
  name = "my_new_gc_role_volume",
  access_mode = "RO",
  bucket = "<bucket-name>",
  description = "GC IAM Role volume",
  configuration = list(
    type = "<type-name>",
    audience = "<audience-link>",
    subject_token_type = "<subject_token_type>",
    service_account_impersonation_url = "<service_account_impersonation_url>",
    token_url = "<token_url>",
    credential_source = list(
      environment_id = "<environment_id>",
      region_url = "<region_url>",
      url = "<url>",
      regional_cred_verification_url = "<regional_cred_verification_url>"
    )
  )
)

# Create Google cloud volume using IAM Role authentication type
# by passing configuration parameter as string path to configuration file
gc_iam_role_volume_config_file <- a$volumes$create_google_using_iam_role(
  name = "my_new_gc_role_volume_cnf_file",
  access_mode = "RO",
  bucket = "<bucket-name>",
  description = "GC IAM Role volume - using config file",
  configuration = "path/to/config/file.json"
)

# Create Google cloud volume using IAM Role authentication type
# using from_path parameter
gc_iam_role_volume_from_path <- a$volumes$create_google_using_iam_role(
  from_path = "path/to/full/config/file.json"
)

# Create Azure volume
azure_volume <- a$volumes$create_azure(
  name = "my_new_azure_volume",
  description = "Azure volume",
  endpoint = "<endpoint>",
  container = "<bucket-name",
  storage_account = "<storage_account-name>",
  tenant_id = "<tenant_id>",
  client_id = "<client_id>",
  client_secret = "<client_secret>",
  resource_id = "<resource_id>"
)

azure_volume_from_path <- a$volumes$create_azure(
  from_path = "path/to/my/json/file.json"
)

# Create Ali cloud volume
ali_volume <- a$volumes$create_ali_oss(
  name = "my_new_azure_volume",
  description = "Ali volume",
  endpoint = "<endpoint>",
  bucket = "<bucket-name",
  access_key_id = "<access_key_id>",
  secret_access_key = "<secret_access_key>"
)

ali_volume_from_path <- a$volumes$create_ali_oss(
  from_path = "path/to/my/json/file.json"
)
```

### Volume object operations

When you've created a new volume, you can notice it is represented as an object
of class Volume. To preview all volume information, use print method:
```{r}
print(aws_iam_user_volume)
```

Within this volume you have following operations available to 
execute:

* `update`: update volume information
* `list_contents` : list volume content
* `get_file`: get single volume file info
* `deactivate` : deactivate volume
* `reactivate` : reactivate previously deactivated volume
* `delete` : delete previously deactivated volume
* `reload` : reload volume object to sync information.

### Update volume

You can update volume's description, access_mode and service information. 
Please, consult our [API documentation]
('https://docs.sevenbridges.com/reference/update-a-volume-v2') 
on how to use service parameter. 
```{r}
# If the volume is created with RO access mode and RO credential parameters,
# and now we want to change it to RW, we should also set proper credential
# parameters that are connected to RW user on the bucket.
# If it's created with RW credentials, but access mode is set to RO, then no
# change is needed in the credentials parameters.
aws_iam_user_volume$update(
  description = "Updated to RW",
  access_mode = "RW",
  service = list(
    credentials = list(
      access_key_id = "<access_key_id_for_rw>",
      secret_access_key = "<secret_access_key_for_rw>",
    )
  )
)
```

### Reload volume

To keep your local Volume object up to date with the volume on the platform, you can always call the `reload()` function:

```{r}
aws_iam_user_volume$reload()
```

### List volume's content

This operation lists all volume files in the root directory of the bucket, 
unless `parent` parameter is specified. In that case, it lists the content of 
that directory on the bucket. 
The output is a `VolumeContentCollection` collection object, that contains 
two fields `items` for storing list of `VolumeFile` objects (files on the volume) 
and `prefixes` for storing list of `VolumePrefix` objects or folders on the 
volume.
You can also specify the `limit` parameter to control the number of results 
returned.
Users can navigate through pages of results by using `continuation token` 
parameter or `link` to fetch next chunk of results. 
If you use `link` parameter, it will overwrite all other parameters if set 
together with it, since it already contains the `limit` and `continuation_token` 
info.

```{r}
# List all files in root bucket directory
content_collection <- aws_iam_user_volume$list_contents(limit = 20)

# Print collection
content_collection

# List all files specific directory on the bucket
folder_files_collection <- aws_iam_user_volume$list_contents(
  prefix = "dir_name"
)

# Get next group of results by setting the continuation token
content_collection <- aws_iam_user_volume$list_contents(
  limit = 20,
  continuation_token = "continuation_token"
)

# Get next group of results by setting the link parameter
aws_iam_user_volume$list_contents(link = "link_to_next_results")

# Or use VolumeContentCollection's object method next_page() for this:
content_collection$next_page()

# You can also fetch all results with all() method
content_collection$all()
```


### Volume files and prefixes

Volume files and prefixes are also treated as objects and they contain some 
operations that could be called on them.

#### Get VolumeFile info

This operation returns a single volume file information. The input parameter 
can be file's id which is represented as location on the bucket 
(`location)`, or a link to that file resource. The link is a `href` field 
of desired file received from response when returning list of volume content 
with `list_contents()`.
Empty arguments are not allowed along with setting both parameters together.
```{r}
# Get single volume file info - by setting file_location
vol_file1 <- aws_iam_user_volume$get_file(location = "file_location_on_bucket")

# Get single volume file info - by setting link
vol_file1 <- aws_iam_user_volume$get_file(link = "full/request/link/to/file")
```

#### Reload VolumeFile object

To keep your local VolumeFile object up to date with the volume file on the 
platform, you can always call the `reload()` function:

```{r}
vol_file1$reload()
```

#### Get VolumePrefix info

Unfortunately we don't have a separate operation to fetch only prefixes on the
volume, therefore, we can only get its prefixes by using the `list_contents()`
operation and look for `prefixes` field in the returned `VolumeContentCollection` 
object. 

```{r}
# List content
volume_content <- aws_iam_user_volume$list_contents()

# Extract prefixes
volume_prefixes <- volume_content$prefixes

# Select one of the volume folders to list its content
volume_folder <- volume_prefixes[[1]]

# Print volume prefix information
volume_folder$print()
```

You can also list the content of a volume prefix/folder on the volume, by
calling `list_contents()` directly on the `VolumePrefix` object.

```{r}
## Select one of the volume folders to list its content
volume_folder <- volume_prefixes[[1]]

# List content
volume_folder_content <- volume_folder$list_contents()
```

### Deactivate and reactivate the volume

Once deactivated, you cannot import from, export to, or browse within a volume. 
As such, the content of the files imported from this volume will no longer be 
accessible on the Platform. However, you can update the volume and manage 
members. 
Note that you cannot deactivate the volume if you have running imports or 
exports unless you force the operation using the query parameter force=true.

Note that to delete a volume, first you must deactivate it and delete all files 
which have been imported from the volume to the Platform.

To reactivate the volume, just use the reactivate() function.

```{r}
# Deactivate volume
aws_iam_user_volume$deactivate()

# Reactivate volume
aws_iam_user_volume$reactivate()
```

### Delete volume

To be able to delete a volume, you first need to deactivate it and then delete 
all files on the Platform that were previously imported from the volume.

```{r}
# Deactivate volume
aws_iam_user_volume$deactivate()

# Delete volume
aws_iam_user_volume$delete()
```

### List volume members

In order to fetch members of one volume or a specific member by its username, 
you can use list_members() and get_member() operations:
```{r}
# List volume members
aws_iam_user_volume$list_members() # limit = 2

# Get single member
aws_iam_user_volume$get_member(user = "member-username")
```

### Remove members

User can remove volume members by providing its username or object of the Member
class to the remove_member() function:
```{r}
# Remove member
aws_iam_user_volume$remove_member("member-username")

# Remove member using Member object
members <- aws_iam_user_volume$list_members()
aws_iam_user_volume$remove_member(members[[3]])
```

### Adding new members

Function for adding new members can accept a Member object (for example used in
some project) or its username. 
```{r}
# Add member via username
aws_iam_user_volume$add_member(user = "member-username", permissions = list(
  read = TRUE, copy = TRUE, write = FALSE, admin = FALSE
))

# Add member via Member object
aws_iam_user_volume$add_member(
  user = Member$new(
    username = "member-username",
    id = "member-username"
  ),
  permissions = list(
    read = TRUE, copy = TRUE, write = FALSE,
    admin = FALSE
  )
)
```

### Modifying member's permissions

Users can modify specific member's permissions on the volume by providing the
privileges they want to change:
```{r}
# Modify member permissions
aws_iam_user_volume$modify_member_permissions(
  user = "member-username",
  permissions = list(write = TRUE)
)
```

## Imports

### List volume imports

Users are able to preview all import jobs they've created when they were trying
to import files/folders from some cloud bucket using volumes, into the platform. 

```{r}
# List imports
all_imports <- a$imports$list_imports()

# Limit results to 5
imp_limit5 <- a$imports$list_imports(limit = 5)

# Load next page of 5 results
imp_limit5$next_page(advance_access = TRUE)

# Load all results at once until last page
imp_limit5$all(advance_access = TRUE)
```

It is possible to use some query parameters as different criteria for filtering
results like volume, project, state etc:

```{r}
# List imports with different criteria
imp_states <- auth$imports$query(state = c("RUNNING", "FAILED"))

imp_project <- auth$imports$query(project = "my-project-id")
```

Listing imports is also available within Project and Volume objects, where 
resulted imports are related to the specific project or volume where they're 
called from.

```{r}
## Get a volume where you want to list all imports
vol1 <- auth$volumes$get(id = "volumes_test_division/volume-name")
vol1$list_imports()

## Get a project object where you want to list imports
test_proj <- auth$projects$get("my-project-id")
test_proj$list_imports()
```


### Get single import job

Similarly like in other resource classes, get() method will return single
import job object when provided with job id.

```{r}
# Get single import
imp_obj <- a$imports$get(id = "import-job-id")
```

### Submit new import - import some volume file into the project

In order to import volume files into the project, users can use submit_import()
method from auth$imports path, or directly on the selected VolumeFile object 
(file they want to import) where this function is also available. 

```{r}
## First, get a volume where you want to import files from
vol1 <- a$volumes$get(id = "volumes_test_division/volume-name")

## Then, get a project object/id where you want to import files
test_proj <- a$projects$get("my-project-id")

## list all volume's files
vol1_content <- vol1$list_contents()

## select one of the volume files
volume_file_import <- vol1_content$items[[3]]

## make file import
imp_job1 <- a$imports$submit_import(
  source_location = volume_file_import,
  destination_project = test_proj,
  autorename = TRUE
)

# alternatively you can also call import() directly on the VolumeFile object
imp_job1 <- volume_file_import$import(
  destination_project = test_proj,
  autorename = TRUE
)

## print import job info
print(imp_job1)
```

One can also import folders from the volume into the project, with option to 
preserve or not to preserve folder structure:

```{r}
## select one of the volume folders to import
volume_folder_import <- vol1_content$prefixes[[1]]

# print import job info
print(imp_job1)

## make folder import
imp_job2 <- a$imports$submit_import(
  source_location = volume_folder_import,
  destination_project = test_proj,
  overwrite = TRUE,
  preserve_folder_structure = TRUE
)

# alternatively you can also call import() directly on the VolumePrefix object
imp_job2 <- volume_folder_import$import(
  destination_project = test_proj,
  overwrite = TRUE,
  preserve_folder_structure = TRUE
)

# print import job info
print(imp_job2)
```

### Reload import job

In order to refresh the import job object and get the up to date info about its state, you can always call the `reload()` function:

```{r}
imp_job1$reload()
```

## Exports

### List file exports into the volumes

Users are able to preview all export jobs they've created when they were trying
to export their files from the Platform into some cloud bucket using volumes. 

```{r}
# List exports
all_exports <- a$exports$list_exports()

# Limit results to 5
exp_limit5 <- a$exports$list_exports(limit = 5)

# Load next page of 5 results
exp_limit5$next_page(advance_access = TRUE)

# List all results until last page
exp_limit5$all()
```

It is possible to use some query parameters as different criteria for filtering
results like volume, state etc:

```{r}
# List exports with different criteria
exp_states <- a$exports$query(state = c("RUNNING", "FAILED"))

exp_volume <- a$exports$query(volume = "my-volume-id")
```

Listing exports is also available within Volume objects, where 
results contains all files exported to that specific volume they're 
called from.

```{r}
## Get a volume where you want to list all exports
vol1 <- a$volumes$get(id = "volumes_test_division/volume-name")
vol1$list_exports()
```


### Get single export job

Similarly like in other resource classes, get() method will return single
export job object when provided with job id.

```{r}
# Get single export
exp_obj <- auth$exports$get(id = "export-job-id")
```

### Submit new export - export some file from the platform into the volume

In order to export platform files into the volumes, users can use 
submit_export() method from auth$exports path, or directly on the selected 
File object (file they want to export) where this function is also available. 

```{r}
## First, get a volume where you want to export files into
vol1 <- a$volumes$get(id = "volumes_test_division/volume-name")

## Then, get a File object/id you want to export from the platform
test_file <- a$files$get("my-file-id")

## make file export
exp_job1 <- a$exports$submit_export(
  source_file = test_file,
  destination_volume = vol1,
  destination_location = "new_volume_file.txt"
)

## print export job info
print(exp_job1)
```

Bear in mind that folders export from the platform into the volumes is not 
possible with this function. For such cases (or multiple files export) it is 
better to use bulk actions.

Users can also export the file into some volume's directory, by providing the 
prefix within the location parameter as a folder name, that will be virtually 
created on the volume:

```{r}
## export file into the folder 'test_folder'
exp_job2 <- a$exports$submit_export(
  source_file = test_file,
  destination_volume = vol1,
  destination_location = "test_folder/new_volume_file.txt"
)

# print export job info
print(exp_job2)
```

### Reload export job

In order to refresh the export job object and get the up to date info about its state, you can always call the `reload()` function:

```{r}
exp_job1$reload()
```
